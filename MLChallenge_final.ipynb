{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"sample.csv\",names=list(range(1,296)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>259.227165</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7059.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>271.983584</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>235.233437</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>415.104389</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>462.230610</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 295 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1    2        3    4    5    6    7    8    9    10  ...   286  287  288  \\\n",
       "0    0    0  20000.0    0    0    1    0    0    0    0 ...     0    0    0   \n",
       "0    0    0   7059.0    0    0    1    0    0    0    0 ...     0    0    0   \n",
       "0    0    0   3150.0    0    0    1    0    0    0    0 ...     0    0    0   \n",
       "0    0    0  24000.0    0    0    1    0    0    0    0 ...     0    0    0   \n",
       "0    0    0   5600.0    0    0    1    0    0    0    0 ...     0    0    0   \n",
       "\n",
       "   289  290  291  292  293         294  295  \n",
       "0    0    0    1    0    0  259.227165    B  \n",
       "0    0    0    0    1    0  271.983584    E  \n",
       "0    0    0    1    0    0  235.233437    D  \n",
       "0    0    0    0    1    0  415.104389    C  \n",
       "0    0    0    0    0    1  462.230610    D  \n",
       "\n",
       "[5 rows x 295 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.014757</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>11226.207294</td>\n",
       "      <td>0.213511</td>\n",
       "      <td>0.049261</td>\n",
       "      <td>0.869498</td>\n",
       "      <td>0.030588</td>\n",
       "      <td>0.005867</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011522</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.007258</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.257269</td>\n",
       "      <td>0.257934</td>\n",
       "      <td>0.179552</td>\n",
       "      <td>0.305245</td>\n",
       "      <td>454.969820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.120581</td>\n",
       "      <td>0.061244</td>\n",
       "      <td>8153.148240</td>\n",
       "      <td>0.609365</td>\n",
       "      <td>0.216415</td>\n",
       "      <td>0.336857</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.076369</td>\n",
       "      <td>0.065044</td>\n",
       "      <td>0.060998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106719</td>\n",
       "      <td>0.051372</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.083556</td>\n",
       "      <td>0.054081</td>\n",
       "      <td>0.437132</td>\n",
       "      <td>0.437501</td>\n",
       "      <td>0.383816</td>\n",
       "      <td>0.460515</td>\n",
       "      <td>221.293898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.525593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>300.863033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9614.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>424.698119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18984.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>575.044994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>72360.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2922.908791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 294 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1             2             3             4             5    \\\n",
       "count  66137.000000  66137.000000  66137.000000  66137.000000  66137.000000   \n",
       "mean       0.014757      0.003765  11226.207294      0.213511      0.049261   \n",
       "std        0.120581      0.061244   8153.148240      0.609365      0.216415   \n",
       "min        0.000000      0.000000     13.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000   4000.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000   9614.800000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000  18984.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000  72360.000000     10.000000      1.000000   \n",
       "\n",
       "                6             7             8             9             10   \\\n",
       "count  66137.000000  66137.000000  66137.000000  66137.000000  66137.000000   \n",
       "mean       0.869498      0.030588      0.005867      0.004249      0.003735   \n",
       "std        0.336857      0.172200      0.076369      0.065044      0.060998   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           ...                285           286           287           288  \\\n",
       "count      ...       66137.000000  66137.000000  66137.000000  66137.000000   \n",
       "mean       ...           0.011522      0.002646      0.007258      0.007031   \n",
       "std        ...           0.106719      0.051372      0.084883      0.083556   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                289           290           291           292           293  \\\n",
       "count  66137.000000  66137.000000  66137.000000  66137.000000  66137.000000   \n",
       "mean       0.002933      0.257269      0.257934      0.179552      0.305245   \n",
       "std        0.054081      0.437132      0.437501      0.383816      0.460515   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      1.000000      1.000000      0.000000      1.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                294  \n",
       "count  66137.000000  \n",
       "mean     454.969820  \n",
       "std      221.293898  \n",
       "min       64.525593  \n",
       "25%      300.863033  \n",
       "50%      424.698119  \n",
       "75%      575.044994  \n",
       "max     2922.908791  \n",
       "\n",
       "[8 rows x 294 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 295 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 295 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[dataset.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Rows with null value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X will have independent variables and Y will be the dependent or target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dataset.iloc[:,:-1]\n",
    "Y=dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    46882\n",
       "D     9279\n",
       "B     6602\n",
       "E     2507\n",
       "A      867\n",
       "Name: 295, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the Target variable from string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just to begin with, trying to fit a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   6,   20,  130,   15,    3],\n",
       "       [   5,  156,  992,  128,    9],\n",
       "       [   9,  320, 8732,  365,   31],\n",
       "       [   5,  108, 1410,  272,   27],\n",
       "       [   1,   43,  342,   75,   24]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.23      0.03      0.06       174\n",
      "           B       0.24      0.12      0.16      1290\n",
      "           C       0.75      0.92      0.83      9457\n",
      "           D       0.32      0.15      0.20      1822\n",
      "           E       0.26      0.05      0.08       485\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     13228\n",
      "   macro avg       0.36      0.26      0.27     13228\n",
      "weighted avg       0.62      0.69      0.64     13228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall are not that great. Probably required to remove some dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcorr=X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcorr=dcorr.abs()\n",
    "dcun=dcorr.unstack()\n",
    "dcunsort=dcun.sort_values(kind=\"quicksort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144  185    1.453386e-08\n",
       "185  144    1.453386e-08\n",
       "166  249    5.926615e-08\n",
       "249  166    5.926615e-08\n",
       "244  117    1.003557e-06\n",
       "117  244    1.003557e-06\n",
       "292  175    1.635319e-06\n",
       "175  292    1.635319e-06\n",
       "35   133    1.743414e-06\n",
       "133  35     1.743414e-06\n",
       "36   153    2.465893e-06\n",
       "153  36     2.465893e-06\n",
       "162  247    2.832268e-06\n",
       "247  162    2.832268e-06\n",
       "96   194    4.113680e-06\n",
       "194  96     4.113680e-06\n",
       "286  196    4.664316e-06\n",
       "196  286    4.664316e-06\n",
       "55   149    5.493499e-06\n",
       "149  55     5.493499e-06\n",
       "86   238    5.917587e-06\n",
       "238  86     5.917587e-06\n",
       "252  92     5.940637e-06\n",
       "92   252    5.940637e-06\n",
       "55   82     6.779026e-06\n",
       "82   55     6.779026e-06\n",
       "196  154    6.981268e-06\n",
       "154  196    6.981268e-06\n",
       "148  182    7.446077e-06\n",
       "182  148    7.446077e-06\n",
       "                ...     \n",
       "292  269             NaN\n",
       "     270             NaN\n",
       "     271             NaN\n",
       "     272             NaN\n",
       "     273             NaN\n",
       "     274             NaN\n",
       "     275             NaN\n",
       "     276             NaN\n",
       "293  59              NaN\n",
       "     179             NaN\n",
       "     268             NaN\n",
       "     269             NaN\n",
       "     270             NaN\n",
       "     271             NaN\n",
       "     272             NaN\n",
       "     273             NaN\n",
       "     274             NaN\n",
       "     275             NaN\n",
       "     276             NaN\n",
       "294  59              NaN\n",
       "     179             NaN\n",
       "     268             NaN\n",
       "     269             NaN\n",
       "     270             NaN\n",
       "     271             NaN\n",
       "     272             NaN\n",
       "     273             NaN\n",
       "     274             NaN\n",
       "     275             NaN\n",
       "     276             NaN\n",
       "Length: 86436, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcunsort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not much correlation. Lets try to remove dimensions which has low variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>36</th>\n",
       "      <th>43</th>\n",
       "      <th>64</th>\n",
       "      <th>294</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>232.000</td>\n",
       "      <td>259.227165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7059.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>136.000</td>\n",
       "      <td>271.983584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>327.000</td>\n",
       "      <td>235.233437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>356.000</td>\n",
       "      <td>415.104389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5600.0</td>\n",
       "      <td>2229.0</td>\n",
       "      <td>5.60</td>\n",
       "      <td>710.000</td>\n",
       "      <td>462.230610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16507.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11.20</td>\n",
       "      <td>779.000</td>\n",
       "      <td>824.520326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9329.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>430.000</td>\n",
       "      <td>559.273602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>304.000</td>\n",
       "      <td>312.010930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>468.203</td>\n",
       "      <td>645.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>430.000</td>\n",
       "      <td>395.571032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>273.000</td>\n",
       "      <td>292.129951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7485.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.40</td>\n",
       "      <td>332.000</td>\n",
       "      <td>293.810666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7884.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.20</td>\n",
       "      <td>404.000</td>\n",
       "      <td>506.651441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>484.000</td>\n",
       "      <td>292.485475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13506.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>499.000</td>\n",
       "      <td>517.928701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>528.000</td>\n",
       "      <td>446.867300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11432.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.40</td>\n",
       "      <td>419.000</td>\n",
       "      <td>418.975922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5979.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>522.000</td>\n",
       "      <td>530.264923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>483.000</td>\n",
       "      <td>361.724558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.60</td>\n",
       "      <td>697.000</td>\n",
       "      <td>598.069814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>17019.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.20</td>\n",
       "      <td>587.000</td>\n",
       "      <td>705.463939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>630.000</td>\n",
       "      <td>313.409217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>184.000</td>\n",
       "      <td>223.444242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>349.000</td>\n",
       "      <td>279.676798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>523.000</td>\n",
       "      <td>774.044890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8180.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>472.000</td>\n",
       "      <td>512.445656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>17245.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>602.000</td>\n",
       "      <td>619.204610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2800.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>323.000</td>\n",
       "      <td>256.474298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>412.000</td>\n",
       "      <td>328.216951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>25191.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.40</td>\n",
       "      <td>406.000</td>\n",
       "      <td>475.067674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66107</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>752.805</td>\n",
       "      <td>900.611737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66108</th>\n",
       "      <td>15000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>557.000</td>\n",
       "      <td>495.459710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66109</th>\n",
       "      <td>5248.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>195.000</td>\n",
       "      <td>466.793022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66110</th>\n",
       "      <td>20395.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>467.000</td>\n",
       "      <td>444.429227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66111</th>\n",
       "      <td>20500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>439.000</td>\n",
       "      <td>494.169427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66112</th>\n",
       "      <td>5825.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.20</td>\n",
       "      <td>177.681</td>\n",
       "      <td>125.648648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66113</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.20</td>\n",
       "      <td>540.000</td>\n",
       "      <td>570.985755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66114</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>566.000</td>\n",
       "      <td>609.870164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66115</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>151.000</td>\n",
       "      <td>206.658514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66116</th>\n",
       "      <td>6251.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.60</td>\n",
       "      <td>460.000</td>\n",
       "      <td>526.830406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66117</th>\n",
       "      <td>23424.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>431.000</td>\n",
       "      <td>493.146667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66118</th>\n",
       "      <td>2800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>826.000</td>\n",
       "      <td>1075.850163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66119</th>\n",
       "      <td>4239.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1882.593</td>\n",
       "      <td>1933.548156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66120</th>\n",
       "      <td>24000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>312.000</td>\n",
       "      <td>384.472253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66121</th>\n",
       "      <td>3700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.20</td>\n",
       "      <td>666.000</td>\n",
       "      <td>690.726321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66122</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>588.000</td>\n",
       "      <td>576.028677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66123</th>\n",
       "      <td>2800.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>1193.000</td>\n",
       "      <td>569.665265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66124</th>\n",
       "      <td>6000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.60</td>\n",
       "      <td>817.733</td>\n",
       "      <td>844.079090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66125</th>\n",
       "      <td>5985.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.50</td>\n",
       "      <td>238.000</td>\n",
       "      <td>349.282231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66126</th>\n",
       "      <td>4588.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>322.000</td>\n",
       "      <td>360.819032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66127</th>\n",
       "      <td>11486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.20</td>\n",
       "      <td>339.000</td>\n",
       "      <td>335.297825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66128</th>\n",
       "      <td>2627.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>216.000</td>\n",
       "      <td>261.188227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66129</th>\n",
       "      <td>17357.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>233.000</td>\n",
       "      <td>350.250564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66130</th>\n",
       "      <td>7763.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>631.000</td>\n",
       "      <td>660.428712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66131</th>\n",
       "      <td>18000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.60</td>\n",
       "      <td>858.000</td>\n",
       "      <td>874.345761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66132</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>720.361</td>\n",
       "      <td>754.125582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66133</th>\n",
       "      <td>13759.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>501.000</td>\n",
       "      <td>521.998666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66134</th>\n",
       "      <td>12100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.40</td>\n",
       "      <td>304.000</td>\n",
       "      <td>430.970745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66135</th>\n",
       "      <td>17280.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.40</td>\n",
       "      <td>612.000</td>\n",
       "      <td>588.470479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66136</th>\n",
       "      <td>1047.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>421.427</td>\n",
       "      <td>377.895620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66137 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           3       36     43        64           294\n",
       "0      20000.0     1.0  13.60   232.000   259.227165\n",
       "1       7059.0     0.0   0.40   136.000   271.983584\n",
       "2       3150.0     1.0   4.40   327.000   235.233437\n",
       "3      24000.0     1.0  13.60   356.000   415.104389\n",
       "4       5600.0  2229.0   5.60   710.000   462.230610\n",
       "5      16507.0     4.0  11.20   779.000   824.520326\n",
       "6       9329.0     1.0  14.00   430.000   559.273602\n",
       "7      24000.0     1.0  13.60   304.000   312.010930\n",
       "8      17000.0     0.0  13.60   468.203   645.829800\n",
       "9       2400.0     3.0   6.00   430.000   395.571032\n",
       "10     25000.0     2.0  13.60   273.000   292.129951\n",
       "11      7485.0     0.0   6.40   332.000   293.810666\n",
       "12      7884.0     1.0  11.20   404.000   506.651441\n",
       "13      1300.0     3.0   4.00   484.000   292.485475\n",
       "14     13506.0     1.0  13.00   499.000   517.928701\n",
       "15      5000.0     1.0   7.00   528.000   446.867300\n",
       "16     11432.0     1.0   8.40   419.000   418.975922\n",
       "17      5979.0     1.0  13.60   522.000   530.264923\n",
       "18      8800.0     1.0   6.00   483.000   361.724558\n",
       "19      3000.0     1.0   6.60   697.000   598.069814\n",
       "20     17019.0     1.0  13.20   587.000   705.463939\n",
       "21     15000.0     1.0  13.60   630.000   313.409217\n",
       "22     23800.0     1.0   6.80   184.000   223.444242\n",
       "23       300.0     0.0   6.00   349.000   279.676798\n",
       "24      5000.0     1.0   7.00   523.000   774.044890\n",
       "25      8180.0     1.0  13.60   472.000   512.445656\n",
       "26     17245.0     1.0   6.80   602.000   619.204610\n",
       "27      2800.0     3.0   1.60   323.000   256.474298\n",
       "28      5000.0     1.0   7.00   412.000   328.216951\n",
       "29     25191.0     1.0  10.40   406.000   475.067674\n",
       "...        ...     ...    ...       ...          ...\n",
       "66107  10000.0     1.0  14.00   752.805   900.611737\n",
       "66108  15000.0     1.0   8.00   557.000   495.459710\n",
       "66109   5248.0     1.0  11.00   195.000   466.793022\n",
       "66110  20395.0     3.0  13.60   467.000   444.429227\n",
       "66111  20500.0     1.0  13.60   439.000   494.169427\n",
       "66112   5825.0     0.0  11.20   177.681   125.648648\n",
       "66113  20000.0     3.0  11.20   540.000   570.985755\n",
       "66114  24000.0     1.0  13.60   566.000   609.870164\n",
       "66115   5000.0     0.0   8.50   151.000   206.658514\n",
       "66116   6251.0     3.0  15.60   460.000   526.830406\n",
       "66117  23424.0     1.0  13.60   431.000   493.146667\n",
       "66118   2800.0     1.0  13.60   826.000  1075.850163\n",
       "66119   4239.0     1.0   2.00  1882.593  1933.548156\n",
       "66120  24000.0     3.0  13.60   312.000   384.472253\n",
       "66121   3700.0     1.0  13.20   666.000   690.726321\n",
       "66122  18000.0     1.0  10.00   588.000   576.028677\n",
       "66123   2800.0     5.0   3.65  1193.000   569.665265\n",
       "66124   6000.0     1.0  11.60   817.733   844.079090\n",
       "66125   5985.0     1.0   9.50   238.000   349.282231\n",
       "66126   4588.0     1.0  13.60   322.000   360.819032\n",
       "66127  11486.0     0.0   7.20   339.000   335.297825\n",
       "66128   2627.0     0.0   5.20   216.000   261.188227\n",
       "66129  17357.0     0.0  13.60   233.000   350.250564\n",
       "66130   7763.0     3.0  13.00   631.000   660.428712\n",
       "66131  18000.0     3.0  13.60   858.000   874.345761\n",
       "66132   5000.0     1.0  10.00   720.361   754.125582\n",
       "66133  13759.0     1.0  10.00   501.000   521.998666\n",
       "66134  12100.0     1.0  10.40   304.000   430.970745\n",
       "66135  17280.0     3.0   6.40   612.000   588.470479\n",
       "66136   1047.0     0.0   1.60   421.427   377.895620\n",
       "\n",
       "[66137 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=X.columns\n",
    "selector=VarianceThreshold(.8)\n",
    "selector.fit_transform(X)\n",
    "features = [columns[x] for x in selector.get_support(indices=True) if x]\n",
    "new_X = pd.DataFrame(selector.transform(X),columns=features)\n",
    "new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled=scaler.transform(new_X)\n",
    "type(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, test_size = 0.20)\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.19      0.04      0.07       204\n",
      "           B       0.24      0.15      0.18      1313\n",
      "           C       0.75      0.90      0.82      9359\n",
      "           D       0.30      0.16      0.21      1876\n",
      "           E       0.23      0.07      0.10       476\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     13228\n",
      "   macro avg       0.34      0.26      0.28     13228\n",
      "weighted avg       0.61      0.68      0.63     13228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, encoded_Y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(13, 13, 13), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    2  158    2    0]\n",
      " [   1    3 1270   10    0]\n",
      " [   0    3 9381   33    0]\n",
      " [   2    1 1877   21    0]\n",
      " [   0    0  446   18    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       162\n",
      "           1       0.33      0.00      0.00      1284\n",
      "           2       0.71      1.00      0.83      9417\n",
      "           3       0.25      0.01      0.02      1901\n",
      "           4       0.00      0.00      0.00       464\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     13228\n",
      "   macro avg       0.26      0.20      0.17     13228\n",
      "weighted avg       0.58      0.71      0.60     13228\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_Y=np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "#encoded_Y = np_utils.to_categorical(encoded_Y)\n",
    "#X_scaled=scaler.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, encoded_Y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=5, units=10)`\n",
      "  \n",
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=5)`\n",
      "  \n",
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52909/52909 [==============================] - 18s 333us/step - loss: 0.3112 - acc: 0.8783\n",
      "Epoch 2/100\n",
      "52909/52909 [==============================] - 16s 309us/step - loss: 0.2961 - acc: 0.8835\n",
      "Epoch 3/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: 0.2947 - acc: 0.8836\n",
      "Epoch 4/100\n",
      "52909/52909 [==============================] - 20s 374us/step - loss: 0.2939 - acc: 0.8835\n",
      "Epoch 5/100\n",
      "52909/52909 [==============================] - 19s 363us/step - loss: 0.2934 - acc: 0.8837\n",
      "Epoch 6/100\n",
      "52909/52909 [==============================] - 21s 388us/step - loss: 0.2928 - acc: 0.8839\n",
      "Epoch 7/100\n",
      "52909/52909 [==============================] - 20s 370us/step - loss: 0.2925 - acc: 0.8839\n",
      "Epoch 8/100\n",
      "52909/52909 [==============================] - 17s 322us/step - loss: 0.2922 - acc: 0.8839\n",
      "Epoch 9/100\n",
      "52909/52909 [==============================] - 17s 322us/step - loss: 0.2918 - acc: 0.8842\n",
      "Epoch 10/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2916 - acc: 0.8844\n",
      "Epoch 11/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2915 - acc: 0.8844\n",
      "Epoch 12/100\n",
      "52909/52909 [==============================] - 17s 316us/step - loss: 0.2914 - acc: 0.8844\n",
      "Epoch 13/100\n",
      "52909/52909 [==============================] - 17s 313us/step - loss: 0.2913 - acc: 0.8845\n",
      "Epoch 14/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2911 - acc: 0.8843\n",
      "Epoch 15/100\n",
      "52909/52909 [==============================] - 17s 313us/step - loss: 0.2912 - acc: 0.8847\n",
      "Epoch 16/100\n",
      "52909/52909 [==============================] - 17s 315us/step - loss: 0.2911 - acc: 0.8846\n",
      "Epoch 17/100\n",
      "52909/52909 [==============================] - 17s 315us/step - loss: 0.2909 - acc: 0.8845\n",
      "Epoch 18/100\n",
      "52909/52909 [==============================] - 19s 352us/step - loss: 0.2910 - acc: 0.8845\n",
      "Epoch 19/100\n",
      "52909/52909 [==============================] - 24s 445us/step - loss: 0.2908 - acc: 0.8846\n",
      "Epoch 20/100\n",
      "52909/52909 [==============================] - 21s 402us/step - loss: 0.2908 - acc: 0.8845\n",
      "Epoch 21/100\n",
      "52909/52909 [==============================] - 21s 401us/step - loss: 0.2907 - acc: 0.8848\n",
      "Epoch 22/100\n",
      "52909/52909 [==============================] - 21s 396us/step - loss: 0.2906 - acc: 0.8848\n",
      "Epoch 23/100\n",
      "52909/52909 [==============================] - 18s 334us/step - loss: 0.2906 - acc: 0.8847\n",
      "Epoch 24/100\n",
      "52909/52909 [==============================] - 17s 315us/step - loss: 0.2906 - acc: 0.8847\n",
      "Epoch 25/100\n",
      "52909/52909 [==============================] - 17s 316us/step - loss: 0.2905 - acc: 0.8846\n",
      "Epoch 26/100\n",
      "52909/52909 [==============================] - 17s 318us/step - loss: 0.2905 - acc: 0.8847\n",
      "Epoch 27/100\n",
      "52909/52909 [==============================] - 18s 334us/step - loss: 0.2905 - acc: 0.8848\n",
      "Epoch 28/100\n",
      "52909/52909 [==============================] - 17s 324us/step - loss: 0.2905 - acc: 0.8846\n",
      "Epoch 29/100\n",
      "52909/52909 [==============================] - 17s 320us/step - loss: 0.2904 - acc: 0.8847\n",
      "Epoch 30/100\n",
      "52909/52909 [==============================] - 17s 321us/step - loss: 0.2903 - acc: 0.8847\n",
      "Epoch 31/100\n",
      "52909/52909 [==============================] - 17s 330us/step - loss: 0.2903 - acc: 0.8847\n",
      "Epoch 32/100\n",
      "52909/52909 [==============================] - 16s 311us/step - loss: 0.2903 - acc: 0.8849\n",
      "Epoch 33/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2903 - acc: 0.8848\n",
      "Epoch 34/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2903 - acc: 0.8847\n",
      "Epoch 35/100\n",
      "52909/52909 [==============================] - 17s 319us/step - loss: 0.2902 - acc: 0.8850\n",
      "Epoch 36/100\n",
      "52909/52909 [==============================] - 17s 321us/step - loss: 0.2902 - acc: 0.8849\n",
      "Epoch 37/100\n",
      "52909/52909 [==============================] - 17s 324us/step - loss: 0.2902 - acc: 0.8849\n",
      "Epoch 38/100\n",
      "52909/52909 [==============================] - 17s 322us/step - loss: 0.2902 - acc: 0.8848\n",
      "Epoch 39/100\n",
      "52909/52909 [==============================] - 17s 325us/step - loss: 0.2900 - acc: 0.8848\n",
      "Epoch 40/100\n",
      "52909/52909 [==============================] - 17s 325us/step - loss: 0.2901 - acc: 0.88510s - loss: 0.2902 - acc: 0.\n",
      "Epoch 41/100\n",
      "52909/52909 [==============================] - 17s 326us/step - loss: 0.2901 - acc: 0.8846\n",
      "Epoch 42/100\n",
      "52909/52909 [==============================] - 17s 327us/step - loss: 0.2900 - acc: 0.8849\n",
      "Epoch 43/100\n",
      "52909/52909 [==============================] - 17s 328us/step - loss: 0.2900 - acc: 0.8848\n",
      "Epoch 44/100\n",
      "52909/52909 [==============================] - 17s 328us/step - loss: 0.2901 - acc: 0.8849\n",
      "Epoch 45/100\n",
      "52909/52909 [==============================] - 17s 328us/step - loss: 0.2899 - acc: 0.8849\n",
      "Epoch 46/100\n",
      "52909/52909 [==============================] - 17s 328us/step - loss: 0.2899 - acc: 0.8849\n",
      "Epoch 47/100\n",
      "52909/52909 [==============================] - 18s 334us/step - loss: 0.2899 - acc: 0.8847\n",
      "Epoch 48/100\n",
      "52909/52909 [==============================] - 18s 346us/step - loss: 0.2898 - acc: 0.8850\n",
      "Epoch 49/100\n",
      "52909/52909 [==============================] - 17s 313us/step - loss: 0.2898 - acc: 0.8849\n",
      "Epoch 50/100\n",
      "52909/52909 [==============================] - 17s 315us/step - loss: 0.2898 - acc: 0.8850\n",
      "Epoch 51/100\n",
      "52909/52909 [==============================] - 17s 313us/step - loss: 0.2898 - acc: 0.8851\n",
      "Epoch 52/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2897 - acc: 0.8848\n",
      "Epoch 53/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2898 - acc: 0.8848\n",
      "Epoch 54/100\n",
      "52909/52909 [==============================] - 17s 312us/step - loss: 0.2898 - acc: 0.8849\n",
      "Epoch 55/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2898 - acc: 0.8848\n",
      "Epoch 56/100\n",
      "52909/52909 [==============================] - 17s 313us/step - loss: 0.2897 - acc: 0.8850\n",
      "Epoch 57/100\n",
      "52909/52909 [==============================] - 17s 315us/step - loss: 0.2897 - acc: 0.8850\n",
      "Epoch 58/100\n",
      "52909/52909 [==============================] - 17s 312us/step - loss: 0.2896 - acc: 0.8850\n",
      "Epoch 59/100\n",
      "52909/52909 [==============================] - 17s 313us/step - loss: 0.2897 - acc: 0.8849\n",
      "Epoch 60/100\n",
      "52909/52909 [==============================] - 17s 313us/step - loss: 0.2897 - acc: 0.8848\n",
      "Epoch 61/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2897 - acc: 0.8849\n",
      "Epoch 62/100\n",
      "52909/52909 [==============================] - 17s 313us/step - loss: 0.2896 - acc: 0.8850\n",
      "Epoch 63/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2897 - acc: 0.8851\n",
      "Epoch 64/100\n",
      "52909/52909 [==============================] - 17s 315us/step - loss: 0.2896 - acc: 0.8851\n",
      "Epoch 65/100\n",
      "52909/52909 [==============================] - 17s 317us/step - loss: 0.2896 - acc: 0.8850\n",
      "Epoch 66/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2897 - acc: 0.8851\n",
      "Epoch 67/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2895 - acc: 0.8850\n",
      "Epoch 68/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2896 - acc: 0.8849\n",
      "Epoch 69/100\n",
      "52909/52909 [==============================] - 17s 317us/step - loss: 0.2895 - acc: 0.8849\n",
      "Epoch 70/100\n",
      "52909/52909 [==============================] - 17s 317us/step - loss: 0.2895 - acc: 0.8850\n",
      "Epoch 71/100\n",
      "52909/52909 [==============================] - 17s 316us/step - loss: 0.2896 - acc: 0.8851\n",
      "Epoch 72/100\n",
      "52909/52909 [==============================] - 17s 317us/step - loss: 0.2894 - acc: 0.8850\n",
      "Epoch 73/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2895 - acc: 0.8849\n",
      "Epoch 74/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2896 - acc: 0.8852\n",
      "Epoch 75/100\n",
      "52909/52909 [==============================] - 18s 339us/step - loss: 0.2895 - acc: 0.8851\n",
      "Epoch 76/100\n",
      "52909/52909 [==============================] - 17s 314us/step - loss: 0.2895 - acc: 0.8850\n",
      "Epoch 77/100\n",
      "52909/52909 [==============================] - 16s 312us/step - loss: 0.2896 - acc: 0.8850\n",
      "Epoch 78/100\n",
      "52909/52909 [==============================] - 16s 310us/step - loss: 0.2894 - acc: 0.8851\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52909/52909 [==============================] - 16s 296us/step - loss: 0.2894 - acc: 0.8851\n",
      "Epoch 80/100\n",
      "52909/52909 [==============================] - 16s 295us/step - loss: 0.2895 - acc: 0.8851\n",
      "Epoch 81/100\n",
      "52909/52909 [==============================] - 16s 294us/step - loss: 0.2895 - acc: 0.8850\n",
      "Epoch 82/100\n",
      "52909/52909 [==============================] - 16s 295us/step - loss: 0.2895 - acc: 0.8851\n",
      "Epoch 83/100\n",
      "52909/52909 [==============================] - 16s 295us/step - loss: 0.2896 - acc: 0.8847\n",
      "Epoch 84/100\n",
      "52909/52909 [==============================] - 16s 295us/step - loss: 0.2895 - acc: 0.8849\n",
      "Epoch 85/100\n",
      "52909/52909 [==============================] - 16s 295us/step - loss: 0.2895 - acc: 0.8848\n",
      "Epoch 86/100\n",
      "52909/52909 [==============================] - 16s 294us/step - loss: 0.2895 - acc: 0.8849\n",
      "Epoch 87/100\n",
      "52909/52909 [==============================] - 16s 295us/step - loss: 0.2894 - acc: 0.8853\n",
      "Epoch 88/100\n",
      "52909/52909 [==============================] - 16s 296us/step - loss: 0.2895 - acc: 0.8848\n",
      "Epoch 89/100\n",
      "52909/52909 [==============================] - 16s 294us/step - loss: 0.2895 - acc: 0.8851\n",
      "Epoch 90/100\n",
      "52909/52909 [==============================] - 16s 297us/step - loss: 0.2894 - acc: 0.8850\n",
      "Epoch 91/100\n",
      "52909/52909 [==============================] - 16s 296us/step - loss: 0.2893 - acc: 0.8851\n",
      "Epoch 92/100\n",
      "52909/52909 [==============================] - 16s 297us/step - loss: 0.2895 - acc: 0.8850\n",
      "Epoch 93/100\n",
      "52909/52909 [==============================] - 16s 296us/step - loss: 0.2894 - acc: 0.8850\n",
      "Epoch 94/100\n",
      "52909/52909 [==============================] - 16s 300us/step - loss: 0.2894 - acc: 0.8850\n",
      "Epoch 95/100\n",
      "52909/52909 [==============================] - 16s 295us/step - loss: 0.2894 - acc: 0.8852\n",
      "Epoch 96/100\n",
      "52909/52909 [==============================] - 16s 296us/step - loss: 0.2894 - acc: 0.8848\n",
      "Epoch 97/100\n",
      "52909/52909 [==============================] - 16s 296us/step - loss: 0.2893 - acc: 0.8849\n",
      "Epoch 98/100\n",
      "52909/52909 [==============================] - 16s 297us/step - loss: 0.2894 - acc: 0.8850\n",
      "Epoch 99/100\n",
      "52909/52909 [==============================] - 16s 296us/step - loss: 0.2893 - acc: 0.8849\n",
      "Epoch 100/100\n",
      "52909/52909 [==============================] - 16s 295us/step - loss: 0.2893 - acc: 0.8850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3231e3c8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 10, activation = 'relu', input_dim = 5))\n",
    "\n",
    "classifier.add(Dense(output_dim = 5, activation = 'softmax'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.4134255e-04, 1.7082270e-02, 9.1010785e-01, 6.2599808e-02,\n",
       "        9.3687242e-03],\n",
       "       [6.1466955e-03, 8.8736176e-02, 7.8835803e-01, 9.9092431e-02,\n",
       "        1.7666683e-02],\n",
       "       [1.8978361e-03, 3.3569016e-02, 8.9678258e-01, 5.9228007e-02,\n",
       "        8.5225822e-03],\n",
       "       ...,\n",
       "       [6.5759257e-03, 9.1104388e-02, 8.0896538e-01, 8.0924124e-02,\n",
       "        1.2430169e-02],\n",
       "       [3.3011676e-03, 6.1633170e-02, 8.3915067e-01, 8.0783069e-02,\n",
       "        1.5131953e-02],\n",
       "       [3.3770117e-03, 6.1976131e-02, 8.0148864e-01, 1.1230380e-01,\n",
       "        2.0854378e-02]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[y_pred]\n",
    "#y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.01      0.01       193\n",
      "           1       0.00      0.00      0.00      1316\n",
      "           2       0.71      1.00      0.83      9351\n",
      "           3       0.46      0.01      0.01      1876\n",
      "           4       0.00      0.00      0.00       492\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     13228\n",
      "   macro avg       0.33      0.20      0.17     13228\n",
      "weighted avg       0.57      0.71      0.59     13228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_classes,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = pd.Series([np.argmax(y, axis=None, out=None) for y in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
