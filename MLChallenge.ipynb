{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"coding_challenge/sample.csv\",header=None,names=list(range(1,296)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8180.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>512.445656</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 295 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1    2       3    4    5    6    7    8    9    10  ...   286  287  288  \\\n",
       "0    0    0  8180.0    0    0    1    0    0    0    0 ...     0    0    0   \n",
       "\n",
       "   289  290  291  292  293         294  295  \n",
       "0    0    0    0    0    1  512.445656    C  \n",
       "\n",
       "[1 rows x 295 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(dataset.head())\n",
    "#print(dataset.tail())\n",
    "dataset.iloc[25,25]=np.NaN\n",
    "dataset=dataset.iloc[:,:-1]\n",
    "dataset[dataset.isnull().any(axis=1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66137, 295)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dataset.iloc[:,:-1]\n",
    "Y=dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    46882\n",
       "3     9279\n",
       "1     6602\n",
       "4     2507\n",
       "0      867\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "#dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "pd.Series(encoded_Y).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66137, 5)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "binencoder = LabelBinarizer()\n",
    "binencoded_Y = binencoder.fit_transform(Y)\n",
    "binencoded_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "      <td>66137.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.014757</td>\n",
       "      <td>0.003765</td>\n",
       "      <td>11226.207294</td>\n",
       "      <td>0.213511</td>\n",
       "      <td>0.049261</td>\n",
       "      <td>0.869498</td>\n",
       "      <td>0.030588</td>\n",
       "      <td>0.005867</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011522</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.007258</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.257269</td>\n",
       "      <td>0.257934</td>\n",
       "      <td>0.179552</td>\n",
       "      <td>0.305245</td>\n",
       "      <td>454.969820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.120581</td>\n",
       "      <td>0.061244</td>\n",
       "      <td>8153.148240</td>\n",
       "      <td>0.609365</td>\n",
       "      <td>0.216415</td>\n",
       "      <td>0.336857</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.076369</td>\n",
       "      <td>0.065044</td>\n",
       "      <td>0.060998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106719</td>\n",
       "      <td>0.051372</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.083556</td>\n",
       "      <td>0.054081</td>\n",
       "      <td>0.437132</td>\n",
       "      <td>0.437501</td>\n",
       "      <td>0.383816</td>\n",
       "      <td>0.460515</td>\n",
       "      <td>221.293898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.525593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>300.863033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9614.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>424.698119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18984.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>575.044994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>72360.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2922.908791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 294 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1             2             3             4             5    \\\n",
       "count  66137.000000  66137.000000  66137.000000  66137.000000  66137.000000   \n",
       "mean       0.014757      0.003765  11226.207294      0.213511      0.049261   \n",
       "std        0.120581      0.061244   8153.148240      0.609365      0.216415   \n",
       "min        0.000000      0.000000     13.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000   4000.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000   9614.800000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000  18984.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000  72360.000000     10.000000      1.000000   \n",
       "\n",
       "                6             7             8             9             10   \\\n",
       "count  66137.000000  66137.000000  66137.000000  66137.000000  66137.000000   \n",
       "mean       0.869498      0.030588      0.005867      0.004249      0.003735   \n",
       "std        0.336857      0.172200      0.076369      0.065044      0.060998   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           ...                285           286           287           288  \\\n",
       "count      ...       66137.000000  66137.000000  66137.000000  66137.000000   \n",
       "mean       ...           0.011522      0.002646      0.007258      0.007031   \n",
       "std        ...           0.106719      0.051372      0.084883      0.083556   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                289           290           291           292           293  \\\n",
       "count  66137.000000  66137.000000  66137.000000  66137.000000  66137.000000   \n",
       "mean       0.002933      0.257269      0.257934      0.179552      0.305245   \n",
       "std        0.054081      0.437132      0.437501      0.383816      0.460515   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      1.000000      1.000000      0.000000      1.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                294  \n",
       "count  66137.000000  \n",
       "mean     454.969820  \n",
       "std      221.293898  \n",
       "min       64.525593  \n",
       "25%      300.863033  \n",
       "50%      424.698119  \n",
       "75%      575.044994  \n",
       "max     2922.908791  \n",
       "\n",
       "[8 rows x 294 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcorr=dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_var=np.where(dcorr>0.8)\n",
    "high_corr_var=[(dcorr.index[x],dcorr.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 44)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_corr_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.040340717140557474"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcorr.iloc[4,44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(dataset.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pca.transform(dataset.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8766.73517775,   -309.03840668],\n",
       "       [ -4169.33581009,     32.16385738],\n",
       "       [ -8076.97719102,    136.55335908],\n",
       "       ...,\n",
       "       [   871.13325645,   -100.18496006],\n",
       "       [  6050.65504582,   -235.20283878],\n",
       "       [-10178.02407469,    191.12254079]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_X_1 = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[:,-1] = labelencoder_X_1.fit_transform(dataset.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=2, units=4, kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52909/52909 [==============================] - 20s 387us/step - loss: -16.9517 - acc: 0.0998\n",
      "Epoch 2/100\n",
      "52909/52909 [==============================] - 27s 506us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 3/100\n",
      "52909/52909 [==============================] - 24s 449us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 4/100\n",
      "52909/52909 [==============================] - 21s 402us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 5/100\n",
      "52909/52909 [==============================] - 21s 392us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 6/100\n",
      "52909/52909 [==============================] - 19s 366us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 7/100\n",
      "52909/52909 [==============================] - 18s 344us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 8/100\n",
      "52909/52909 [==============================] - 19s 361us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 9/100\n",
      "52909/52909 [==============================] - 19s 352us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 10/100\n",
      "52909/52909 [==============================] - 19s 354us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 11/100\n",
      "52909/52909 [==============================] - 19s 353us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 12/100\n",
      "52909/52909 [==============================] - 21s 391us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 13/100\n",
      "52909/52909 [==============================] - 23s 430us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 14/100\n",
      "52909/52909 [==============================] - 21s 388us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 15/100\n",
      "52909/52909 [==============================] - 19s 364us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 16/100\n",
      "52909/52909 [==============================] - 19s 363us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 17/100\n",
      "52909/52909 [==============================] - 20s 376us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 18/100\n",
      "52909/52909 [==============================] - 19s 364us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 19/100\n",
      "52909/52909 [==============================] - 19s 365us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 20/100\n",
      "52909/52909 [==============================] - 19s 359us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 21/100\n",
      "52909/52909 [==============================] - 19s 363us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 22/100\n",
      "52909/52909 [==============================] - 19s 366us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 23/100\n",
      "52909/52909 [==============================] - 22s 409us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 24/100\n",
      "52909/52909 [==============================] - 26s 493us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 25/100\n",
      "52909/52909 [==============================] - 23s 439us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 26/100\n",
      "52909/52909 [==============================] - 19s 356us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 27/100\n",
      "52909/52909 [==============================] - 19s 352us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 28/100\n",
      "52909/52909 [==============================] - 18s 344us/step - loss: -17.3682 - acc: 0.10001s - \n",
      "Epoch 29/100\n",
      "52909/52909 [==============================] - 19s 357us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 30/100\n",
      "52909/52909 [==============================] - 18s 349us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 31/100\n",
      "52909/52909 [==============================] - 19s 351us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 32/100\n",
      "52909/52909 [==============================] - 19s 350us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 33/100\n",
      "52909/52909 [==============================] - 19s 352us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 34/100\n",
      "52909/52909 [==============================] - 19s 356us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 35/100\n",
      "52909/52909 [==============================] - 19s 352us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 36/100\n",
      "52909/52909 [==============================] - 19s 351us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 37/100\n",
      "52909/52909 [==============================] - 19s 353us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 38/100\n",
      "52909/52909 [==============================] - 19s 354us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 39/100\n",
      "52909/52909 [==============================] - 19s 353us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 40/100\n",
      "52909/52909 [==============================] - 19s 355us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 41/100\n",
      "52909/52909 [==============================] - 22s 417us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 42/100\n",
      "52909/52909 [==============================] - 21s 394us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 43/100\n",
      "52909/52909 [==============================] - 19s 355us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 44/100\n",
      "52909/52909 [==============================] - 19s 354us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 45/100\n",
      "52909/52909 [==============================] - 19s 353us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 46/100\n",
      "52909/52909 [==============================] - 19s 357us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 47/100\n",
      "52909/52909 [==============================] - 18s 342us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 48/100\n",
      "52909/52909 [==============================] - 18s 345us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 49/100\n",
      "52909/52909 [==============================] - 18s 339us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 50/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 51/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 52/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 53/100\n",
      "52909/52909 [==============================] - 18s 342us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 54/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 55/100\n",
      "52909/52909 [==============================] - 18s 342us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 56/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 57/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 58/100\n",
      "52909/52909 [==============================] - 18s 339us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 59/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 60/100\n",
      "52909/52909 [==============================] - 18s 342us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 61/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 62/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 63/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 64/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 65/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 66/100\n",
      "52909/52909 [==============================] - 18s 343us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 67/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 68/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 69/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 70/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 71/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 72/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 73/100\n",
      "52909/52909 [==============================] - 18s 343us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 74/100\n",
      "52909/52909 [==============================] - 18s 339us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 75/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 76/100\n",
      "52909/52909 [==============================] - 18s 343us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 77/100\n",
      "52909/52909 [==============================] - 18s 340us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52909/52909 [==============================] - 17s 328us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 79/100\n",
      "52909/52909 [==============================] - 17s 327us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 80/100\n",
      "52909/52909 [==============================] - 18s 331us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 81/100\n",
      "52909/52909 [==============================] - 18s 331us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 82/100\n",
      "52909/52909 [==============================] - 17s 327us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 83/100\n",
      "52909/52909 [==============================] - 17s 328us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 84/100\n",
      "52909/52909 [==============================] - 17s 330us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 85/100\n",
      "52909/52909 [==============================] - 17s 327us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 86/100\n",
      "52909/52909 [==============================] - 17s 329us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 87/100\n",
      "52909/52909 [==============================] - 17s 328us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 88/100\n",
      "52909/52909 [==============================] - 18s 331us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 89/100\n",
      "52909/52909 [==============================] - 17s 327us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 90/100\n",
      "52909/52909 [==============================] - 17s 327us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 91/100\n",
      "52909/52909 [==============================] - 17s 327us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 92/100\n",
      "52909/52909 [==============================] - 17s 327us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 93/100\n",
      "52909/52909 [==============================] - 17s 330us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 94/100\n",
      "52909/52909 [==============================] - 18s 332us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 95/100\n",
      "52909/52909 [==============================] - 20s 386us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 96/100\n",
      "52909/52909 [==============================] - 18s 344us/step - loss: -17.3682 - acc: 0.10000s - loss: -17.3623 - \n",
      "Epoch 97/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 98/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 99/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n",
      "Epoch 100/100\n",
      "52909/52909 [==============================] - 18s 341us/step - loss: -17.3682 - acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1281fe0f0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 4, init = 'uniform', activation = 'relu', input_dim = 2))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, Y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Users/ajaysingh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49602, 294)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    6,    35,   139,    25,     2],\n",
       "       [   12,   211,  1252,   148,    13],\n",
       "       [   14,   386, 10854,   452,    40],\n",
       "       [    5,   151,  1811,   342,    25],\n",
       "       [    2,    60,   433,    79,    38]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    11746\n",
       "3     2334\n",
       "1     1636\n",
       "4      612\n",
       "0      207\n",
       "Name: 295, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13228, 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   3,   24,  111,   15,    6],\n",
       "       [  15,  149,  947,  158,   43],\n",
       "       [  72,  675, 7686,  793,  172],\n",
       "       [  26,  176, 1313,  291,   57],\n",
       "       [  12,   64,  321,   69,   30]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    9398\n",
       "3    1863\n",
       "1    1312\n",
       "4     496\n",
       "0     159\n",
       "Name: 295, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1726.61670089,   -31.45309453],\n",
       "       [-8234.16800849,   142.69639781],\n",
       "       [-5226.95513506,    63.15336384],\n",
       "       ...,\n",
       "       [ 2095.90175357,  -132.56973883],\n",
       "       [-8225.85047457,   142.48451033],\n",
       "       [12765.79346431,  -414.83787791]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
